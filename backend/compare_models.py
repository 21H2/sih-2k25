#!/usr/bin/env python3
"""
Compare Traditional RandomForest vs LLM responses
"""

import os
from dotenv import load_dotenv
from whatsapp_bot import MedicalChatbot
from llm_integration import LLMManager

load_dotenv()

def compare_responses():
    """Compare traditional vs LLM responses side by side"""
    
    print("ğŸ†š Traditional RandomForest vs LLM Comparison\n")
    print("=" * 80)
    
    # Initialize both systems
    chatbot = MedicalChatbot()
    llm_manager = LLMManager()
    
    # Test queries
    test_queries = [
        "I have fever and headache",
        "My stomach hurts after eating",
        "I'm having chest pain during exercise",
        "My child has a rash on their arms",
        "I'm diabetic and my wound isn't healing"
    ]
    
    for i, query in enumerate(test_queries, 1):
        print(f"\nğŸ“‹ Test {i}: {query}")
        print("-" * 80)
        
        # Traditional RandomForest Response
        print("ğŸ”¹ TRADITIONAL MODEL (RandomForest):")
        try:
            # Temporarily disable LLM to get traditional response
            original_use_llm = chatbot.use_llm
            chatbot.use_llm = False
            
            traditional_response = chatbot.get_medical_advice(query)
            print(f"   {traditional_response[:200]}...")
            
            # Restore LLM setting
            chatbot.use_llm = original_use_llm
            
        except Exception as e:
            print(f"   âŒ Error: {e}")
        
        print()
        
        # LLM Response
        print("ğŸ¤– LLM MODEL (GPT/Claude):")
        try:
            if llm_manager.is_available():
                llm_response = llm_manager.generate_response(query)
                if llm_response:
                    print(f"   {llm_response[:200]}...")
                else:
                    print("   âŒ No response generated")
            else:
                print("   âŒ No LLM providers configured")
                
        except Exception as e:
            print(f"   âŒ Error: {e}")
        
        print("\n" + "=" * 80)
    
    # Summary
    print("\nğŸ“Š COMPARISON SUMMARY:")
    print("\nğŸ”¹ Traditional RandomForest:")
    print("   âœ… Fast (instant response)")
    print("   âœ… Free (no API costs)")
    print("   âœ… Reliable (always works)")
    print("   âŒ Limited responses (pre-trained only)")
    print("   âŒ No context understanding")
    print("   âŒ Can't handle complex queries")
    
    print("\nğŸ¤– LLM (GPT/Claude):")
    print("   âœ… Intelligent (understands context)")
    print("   âœ… Dynamic responses (generates new answers)")
    print("   âœ… Handles complex medical scenarios")
    print("   âœ… Conversational (can ask follow-up questions)")
    print("   âŒ Slower (2-3 seconds)")
    print("   âŒ Costs money (~$0.01 per message)")
    print("   âŒ Requires internet/API")
    
    print("\nğŸ’¡ RECOMMENDATION:")
    print("   Use HYBRID approach (current setup):")
    print("   - LLM for complex queries")
    print("   - RandomForest as fallback")
    print("   - Best of both worlds!")

if __name__ == "__main__":
    compare_responses()